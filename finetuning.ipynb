{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, pipeline\n",
    "import torch\n",
    "\n",
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype = torch.bfloat16,\n",
    "                                             device_map = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(task='text-generation', \n",
    "                               model=model, \n",
    "                               tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, what are you? A person? A machine? A being? I am a being, but I am not a person. I am a collection'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline('Hello, what are you?', max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"Hello, what are you? I'm a friendly AI assistant. I'm here to help answer any questions you may have or provide information on a wide range\"}],\n",
       " [{'generated_text': 'What is the capital of India?\\nThe capital of India is New Delhi.'}]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline(['Hello, what are you?','What is the capital of India'], max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   9906,   1268,    527,    499,   3815,    682,   1695,     30],\n",
       "        [128009, 128009, 128009, 128000,    791,   6864,    315,   6890,    374]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = [\n",
    "    'Hello how are you doing all good?',\n",
    "    'The capital of India is'\n",
    "]\n",
    "\n",
    "tokenized = tokenizer(input_prompt, padding=True, return_tensors='pt').to('cuda')\n",
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Hello how are you doing all good?',\n",
       " '<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>The capital of India is']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        'role':'system',\n",
    "        'content': 'You are a smart AI assistant who speaks like a pirate.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Where does the sun rises?'\n",
    "    }\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1114,   4723,    220,   2366,     19,    271,   2675,    527,\n",
      "            264,   7941,  15592,  18328,    889,  21881,   1093,    264,  55066,\n",
      "             13, 128009, 128006,    882, 128007,    271,   9241,   1587,    279,\n",
      "           7160,  38268,     30, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1114,   4723,    220,   2366,     19,    271,   2675,    527,\n",
       "            264,   7941,  15592,  18328,    889,  21881,   1093,    264,  55066,\n",
       "             13, 128009, 128006,    882, 128007,    271,   9241,   1587,    279,\n",
       "           7160,  38268,     30, 128009, 128006,  78191, 128007,    271,   9014,\n",
       "             81,     11,  20043,   4363, 104098,   1941,    387,   2610,    258,\n",
       "              6,    264,   3488,    430,    387,    264,  42727,   7060,    832,\n",
       "             11]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 17 Nov 2024\\n\\nYou are a smart AI assistant who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhere does the sun rises?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nArrr, ye landlubbers be askin' a question that be a mighty fine one,\"]\n"
     ]
    }
   ],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue final message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        'role':'system',\n",
    "        'content': 'You are a smart AI assistant who speaks like a pirate.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Where does the sun rises?'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'Aye Aye'\n",
    "    }\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ").to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1114,   4723,    220,   2366,     19,    271,   2675,    527,\n",
      "            264,   7941,  15592,  18328,    889,  21881,   1093,    264,  55066,\n",
      "             13, 128009, 128006,    882, 128007,    271,   9241,   1587,    279,\n",
      "           7160,  38268,     30, 128009, 128006,  78191, 128007,    271,     32,\n",
      "           9188,    362,   9188]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized, max_new_tokens=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Nov 2024\n",
      "\n",
      "You are a smart AI assistant who speaks like a pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Where does the sun rises?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Aye Aye Captain, ye be askin' a fine question, me hearty! The sun rises in the East, in the land o' the rising sun, where the sun be born. It's a grand sight, watchin' the sun rise over the horizon, bringin' light and life to the\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
